<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  
  <link rel="shortcut icon" href="../../../img/favicon.ico">
  <title>Persistent Advantage Learning - Reinforcement Learning Coach</title>
  <link href='https://fonts.googleapis.com/css?family=Lato:400,700|Roboto+Slab:400,700|Inconsolata:400,700' rel='stylesheet' type='text/css'>

  <link rel="stylesheet" href="../../../css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../css/theme_extra.css" type="text/css" />
  <link rel="stylesheet" href="../../../css/highlight.css">
  <link href="../../../extra.css" rel="stylesheet">
  
  <script>
    // Current page data
    var mkdocs_page_name = "Persistent Advantage Learning";
    var mkdocs_page_input_path = "algorithms/value_optimization/pal.md";
    var mkdocs_page_url = "/algorithms/value_optimization/pal/";
  </script>
  
  <script src="../../../js/jquery-2.1.1.min.js"></script>
  <script src="../../../js/modernizr-2.8.3.min.js"></script>
  <script type="text/javascript" src="../../../js/highlight.pack.js"></script> 
  
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
      <div class="wy-side-nav-search">
        <a href="../../.." class="icon icon-home"> Reinforcement Learning Coach</a>
        <div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
	<ul class="current">
	  
          
            <li class="toctree-l1">
		
    <a class="" href="../../..">Home</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../../../usage/">Usage</a>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">Design</span>
    <ul class="subnav">
                <li class="">
                    
    <a class="" href="../../../design/features/">Features</a>
                </li>
                <li class="">
                    
    <a class="" href="../../../design/control_flow/">Control Flow</a>
                </li>
                <li class="">
                    
    <a class="" href="../../../design/network/">Network</a>
                </li>
                <li class="">
                    
    <a class="" href="../../../design/filters/">Filters</a>
                </li>
    </ul>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">Algorithms</span>
    <ul class="subnav">
                <li class="">
                    
    <a class="" href="../dqn/">DQN</a>
                </li>
                <li class="">
                    
    <a class="" href="../double_dqn/">Double DQN</a>
                </li>
                <li class="">
                    
    <a class="" href="../dueling_dqn/">Dueling DQN</a>
                </li>
                <li class="">
                    
    <a class="" href="../categorical_dqn/">Categorical DQN</a>
                </li>
                <li class="">
                    
    <a class="" href="../mmc/">Mixed Monte Carlo</a>
                </li>
                <li class=" current">
                    
    <a class="current" href="./">Persistent Advantage Learning</a>
    <ul class="subnav">
            
    <li class="toctree-l3"><a href="#persistent-advantage-learning">Persistent Advantage Learning</a></li>
    
        <ul>
        
            <li><a class="toctree-l4" href="#network-structure">Network Structure</a></li>
        
            <li><a class="toctree-l4" href="#algorithm-description">Algorithm Description</a></li>
        
        </ul>
    

    </ul>
                </li>
                <li class="">
                    
    <a class="" href="../nec/">Neural Episodic Control</a>
                </li>
                <li class="">
                    
    <a class="" href="../bs_dqn/">Bootstrapped DQN</a>
                </li>
                <li class="">
                    
    <a class="" href="../n_step/">N-Step Q Learning</a>
                </li>
                <li class="">
                    
    <a class="" href="../naf/">Normalized Advantage Functions</a>
                </li>
                <li class="">
                    
    <a class="" href="../../policy_optimization/pg/">Policy Gradient</a>
                </li>
                <li class="">
                    
    <a class="" href="../../policy_optimization/ac/">Actor-Critic</a>
                </li>
                <li class="">
                    
    <a class="" href="../../policy_optimization/ddpg/">Deep Determinstic Policy Gradients</a>
                </li>
                <li class="">
                    
    <a class="" href="../../policy_optimization/ppo/">Proximal Policy Optimization</a>
                </li>
                <li class="">
                    
    <a class="" href="../../policy_optimization/cppo/">Clipped Proximal Policy Optimization</a>
                </li>
                <li class="">
                    
    <a class="" href="../../other/dfp/">Direct Future Prediction</a>
                </li>
                <li class="">
                    
    <a class="" href="../../imitation/bc/">Behavioral Cloning</a>
                </li>
    </ul>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../../../dashboard/">Coach Dashboard</a>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">Contributing</span>
    <ul class="subnav">
                <li class="">
                    
    <a class="" href="../../../contributing/add_agent/">Adding a New Agent</a>
                </li>
                <li class="">
                    
    <a class="" href="../../../contributing/add_env/">Adding a New Environment</a>
                </li>
    </ul>
	    </li>
          
        </ul>
      </div>
      &nbsp;
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
        <a href="../../..">Reinforcement Learning Coach</a>
      </nav>

      
      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="../../..">Docs</a> &raquo;</li>
    
      
        
          <li>Algorithms &raquo;</li>
        
      
    
    <li>Persistent Advantage Learning</li>
    <li class="wy-breadcrumbs-aside">
      
    </li>
  </ul>
  <hr/>
</div>
          <div role="main">
            <div class="section">
              
                <h1 id="persistent-advantage-learning">Persistent Advantage Learning</h1>
<p><strong>Actions space:</strong> Discrete</p>
<p><strong>References:</strong> <a href="https://arxiv.org/abs/1512.04860">Increasing the Action Gap: New Operators for Reinforcement Learning</a></p>
<h2 id="network-structure">Network Structure</h2>
<p style="text-align: center;">

<img src="../../design_imgs/dqn.png">

</p>

<h2 id="algorithm-description">Algorithm Description</h2>
<h3 id="training-the-network">Training the network</h3>
<ol>
<li>
<p>Sample a batch of transitions from the replay buffer. </p>
</li>
<li>
<p>Start by calculating the initial target values in the same manner as they are calculated in DDQN
   <script type="math/tex; mode=display"> y_t^{DDQN}=r(s_t,a_t )+\gamma Q(s_{t+1},argmax_a Q(s_{t+1},a)) </script>
</p>
</li>
<li>The action gap <script type="math/tex"> V(s_t )-Q(s_t,a_t) </script> should then be subtracted from each of the calculated targets. To calculate the action gap, run the target network using the current states and get the <script type="math/tex"> Q </script> values for all the actions. Then estimate <script type="math/tex"> V </script> as the maximum predicted <script type="math/tex"> Q </script> value for the current state:
   <script type="math/tex; mode=display"> V(s_t )=max_a Q(s_t,a) </script>
</li>
<li>For <em>advantage learning (AL)</em>, reduce the action gap weighted by a predefined parameter <script type="math/tex"> \alpha </script> from the targets <script type="math/tex"> y_t^{DDQN} </script>: 
   <script type="math/tex; mode=display"> y_t=y_t^{DDQN}-\alpha \cdot (V(s_t )-Q(s_t,a_t )) </script>
</li>
<li>For <em>persistent advantage learning (PAL)</em>, the target network is also used in order to calculate the action gap for the next state:
   <script type="math/tex; mode=display"> V(s_{t+1} )-Q(s_{t+1},a_{t+1}) </script>
   where <script type="math/tex"> a_{t+1} </script> is chosen by running the next states through the online network and choosing the action that has the highest predicted <script type="math/tex"> Q </script> value. Finally, the targets will be defined as -
   <script type="math/tex; mode=display"> y_t=y_t^{DDQN}-\alpha \cdot min(V(s_t )-Q(s_t,a_t ),V(s_{t+1} )-Q(s_{t+1},a_{t+1} )) </script>
</li>
<li>
<p>Train the online network using the current states as inputs, and with the aforementioned targets.</p>
</li>
<li>
<p>Once in every few thousand steps, copy the weights from the online network to the target network.</p>
</li>
</ol>
              
            </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="../nec/" class="btn btn-neutral float-right" title="Neural Episodic Control">Next <span class="icon icon-circle-arrow-right"></span></a>
      
      
        <a href="../mmc/" class="btn btn-neutral" title="Mixed Monte Carlo"><span class="icon icon-circle-arrow-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
    
  </div>

  Built with <a href="http://www.mkdocs.org">MkDocs</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
      
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" style="cursor: pointer">
    <span class="rst-current-version" data-toggle="rst-current-version">
      
      
        <span><a href="../mmc/" style="color: #fcfcfc;">&laquo; Previous</a></span>
      
      
        <span style="margin-left: 15px"><a href="../nec/" style="color: #fcfcfc">Next &raquo;</a></span>
      
    </span>
</div>
    <script>var base_url = '../../..';</script>
    <script src="../../../js/theme.js"></script>
      <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML"></script>
      <script src="../../../search/require.js"></script>
      <script src="../../../search/search.js"></script>

</body>
</html>
