{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started Guide"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "- [Using Coach from the Command Line](#Using-Coach-from-the-Command-Line)\n",
    "- [Using Coach as a Library](#Using-Coach-as-a-Library)\n",
    "    - [Preset based - using `CoachInterface`](#Preset-based---using-CoachInterface)\n",
    "        - [Training a preset](#Training-a-preset)\n",
    "        - [Running each training or inference iteration manually](#Running-each-training-or-inference-iteration-manually)\n",
    "    - [Non-preset - using `GraphManager` directly](#Non-preset---using-GraphManager-directly)\n",
    "        - [Training an agent with a custom Gym environment](#Training-an-agent-with-a-custom-Gym-environment)\n",
    "        - [Advanced functionality - proprietary exploration policy, checkpoint evaluation](#Advanced-functionality---proprietary-exploration-policy,-checkpoint-evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Coach from the Command Line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When running Coach from the command line, we use a Preset module to define the experiment parameters.\n",
    "As its name implies, a preset is a predefined set of parameters to run some agent on some environment.\n",
    "Coach has many predefined presets that follow the algorithms definitions in the published papers, and allows training some of the existing algorithms with essentially no coding at all. This presets can easily be run from the command line. For example:\n",
    "\n",
    "`coach -p CartPole_DQN`\n",
    "\n",
    "You can find all the predefined presets under the `presets` directory, or by listing them using the following command:\n",
    "\n",
    "`coach -l`\n",
    "\n",
    "Coach can also be used with an externally defined preset by passing the absolute path to the module and the name of the graph manager object which is defined in the preset: \n",
    "\n",
    "`coach -p /home/my_user/my_agent_dir/my_preset.py:graph_manager`\n",
    "\n",
    "Some presets are generic for multiple environment levels, and therefore require defining the specific level through the command line:\n",
    "\n",
    "`coach -p Atari_DQN -lvl breakout`\n",
    "\n",
    "There are plenty of other command line arguments you can use in order to customize the experiment. A full documentation of the available arguments can be found using the following command:\n",
    "\n",
    "`coach -h`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Coach as a Library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, Coach can be used a library directly from python. As described above, Coach uses the presets mechanism to define the experiments. A preset is essentially a python module which instantiates a `GraphManager` object. The graph manager is a container that holds the agents and the environments, and has some additional parameters for running the experiment, such as visualization parameters. The graph manager acts as the scheduler which orchestrates the experiment.\n",
    "\n",
    "**Note: Each one of the examples in this section is independent, so notebook kernels need to be restarted before running it. Make sure you run the next cell before running any of the examples.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding module path to sys path if not there, so rl_coach submodules can be imported\n",
    "import os\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "resources_path = os.path.abspath(os.path.join('Resources'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "if resources_path not in sys.path:\n",
    "    sys.path.append(resources_path)\n",
    "    \n",
    "from rl_coach.coach import CoachInterface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preset based - using `CoachInterface`\n",
    "\n",
    "The basic method to run Coach directly from python is  through a `CoachInterface` object, which uses the same arguments as the command line invocation but allowes for more flexibility and additional control of the training/inference process.\n",
    "\n",
    "Let's start with some examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training a preset\n",
    "In this example, we'll create a very simple graph containing a Clipped PPO agent running with the CartPole-v0 Gym environment. `CoachInterface` has a few useful parameters such as `custom_parameter` that enables overriding preset settings, and other optional parameters enabling control over the training process. We'll override the preset's schedule parameters, train with a single rollout worker, and save checkpoints every 10 seconds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coach = CoachInterface(preset='CartPole_ClippedPPO',\n",
    "                       # The optional custom_parameter enables overriding preset settings\n",
    "                       custom_parameter='heatup_steps=EnvironmentSteps(5);improve_steps=TrainingSteps(3)',\n",
    "                       # Other optional parameters enable easy access to advanced functionalities\n",
    "                       num_workers=1, checkpoint_save_secs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b]2;\u0007\n",
      "\u001b[30;46msimple_rl_graph: Starting heatup\u001b[0m\n",
      "\u001b[95mHeatup\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m1 \u001b[94mTotal reward: \u001b[0m19.0 \u001b[94mSteps: \u001b[0m19 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[30;46mStarting to improve simple_rl_graph task index 0\u001b[0m\n",
      "INFO:tensorflow:./experiments/17_09_2019-13_08/checkpoint/0_Step-1.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "\u001b[95mCheckpoint\u001b[0m - \u001b[94mSaving in path: \u001b[0m['./experiments/17_09_2019-13_08/checkpoint/0_Step-1.ckpt'] \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m2 \u001b[94mTotal reward: \u001b[0m51.0 \u001b[94mSteps: \u001b[0m70 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m3 \u001b[94mTotal reward: \u001b[0m16.0 \u001b[94mSteps: \u001b[0m86 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m4 \u001b[94mTotal reward: \u001b[0m42.0 \u001b[94mSteps: \u001b[0m128 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m5 \u001b[94mTotal reward: \u001b[0m69.0 \u001b[94mSteps: \u001b[0m197 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m6 \u001b[94mTotal reward: \u001b[0m53.0 \u001b[94mSteps: \u001b[0m250 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m7 \u001b[94mTotal reward: \u001b[0m22.0 \u001b[94mSteps: \u001b[0m272 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m8 \u001b[94mTotal reward: \u001b[0m47.0 \u001b[94mSteps: \u001b[0m319 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m9 \u001b[94mTotal reward: \u001b[0m34.0 \u001b[94mSteps: \u001b[0m353 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m10 \u001b[94mTotal reward: \u001b[0m97.0 \u001b[94mSteps: \u001b[0m450 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m11 \u001b[94mTotal reward: \u001b[0m29.0 \u001b[94mSteps: \u001b[0m479 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m12 \u001b[94mTotal reward: \u001b[0m74.0 \u001b[94mSteps: \u001b[0m553 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m13 \u001b[94mTotal reward: \u001b[0m38.0 \u001b[94mSteps: \u001b[0m591 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m14 \u001b[94mTotal reward: \u001b[0m14.0 \u001b[94mSteps: \u001b[0m605 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m15 \u001b[94mTotal reward: \u001b[0m97.0 \u001b[94mSteps: \u001b[0m702 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m16 \u001b[94mTotal reward: \u001b[0m49.0 \u001b[94mSteps: \u001b[0m751 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m17 \u001b[94mTotal reward: \u001b[0m22.0 \u001b[94mSteps: \u001b[0m773 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m18 \u001b[94mTotal reward: \u001b[0m18.0 \u001b[94mSteps: \u001b[0m791 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m19 \u001b[94mTotal reward: \u001b[0m84.0 \u001b[94mSteps: \u001b[0m875 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m20 \u001b[94mTotal reward: \u001b[0m52.0 \u001b[94mSteps: \u001b[0m927 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m21 \u001b[94mTotal reward: \u001b[0m44.0 \u001b[94mSteps: \u001b[0m971 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m22 \u001b[94mTotal reward: \u001b[0m28.0 \u001b[94mSteps: \u001b[0m999 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m23 \u001b[94mTotal reward: \u001b[0m22.0 \u001b[94mSteps: \u001b[0m1021 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m24 \u001b[94mTotal reward: \u001b[0m104.0 \u001b[94mSteps: \u001b[0m1125 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m25 \u001b[94mTotal reward: \u001b[0m76.0 \u001b[94mSteps: \u001b[0m1201 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m26 \u001b[94mTotal reward: \u001b[0m128.0 \u001b[94mSteps: \u001b[0m1329 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m27 \u001b[94mTotal reward: \u001b[0m14.0 \u001b[94mSteps: \u001b[0m1343 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m28 \u001b[94mTotal reward: \u001b[0m99.0 \u001b[94mSteps: \u001b[0m1442 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m29 \u001b[94mTotal reward: \u001b[0m58.0 \u001b[94mSteps: \u001b[0m1500 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m30 \u001b[94mTotal reward: \u001b[0m95.0 \u001b[94mSteps: \u001b[0m1595 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m31 \u001b[94mTotal reward: \u001b[0m36.0 \u001b[94mSteps: \u001b[0m1631 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m32 \u001b[94mTotal reward: \u001b[0m48.0 \u001b[94mSteps: \u001b[0m1679 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m33 \u001b[94mTotal reward: \u001b[0m158.0 \u001b[94mSteps: \u001b[0m1837 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m34 \u001b[94mTotal reward: \u001b[0m70.0 \u001b[94mSteps: \u001b[0m1907 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m35 \u001b[94mTotal reward: \u001b[0m93.0 \u001b[94mSteps: \u001b[0m2000 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "INFO:tensorflow:./experiments/17_09_2019-13_08/checkpoint/1_Step-1986.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "\u001b[95mCheckpoint\u001b[0m - \u001b[94mSaving in path: \u001b[0m['./experiments/17_09_2019-13_08/checkpoint/1_Step-1986.ckpt'] \n",
      "\u001b[30;46magent: Starting evaluation phase\u001b[0m\n",
      "\u001b[95mTesting\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m35 \u001b[94mTotal reward: \u001b[0m141.0 \u001b[94mSteps: \u001b[0m2067 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTesting\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m35 \u001b[94mTotal reward: \u001b[0m130.0 \u001b[94mSteps: \u001b[0m2067 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTesting\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m35 \u001b[94mTotal reward: \u001b[0m145.0 \u001b[94mSteps: \u001b[0m2067 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTesting\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m35 \u001b[94mTotal reward: \u001b[0m105.0 \u001b[94mSteps: \u001b[0m2067 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTesting\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m35 \u001b[94mTotal reward: \u001b[0m198.0 \u001b[94mSteps: \u001b[0m2067 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[30;46magent: Finished evaluation phase. Success rate = 0.0, Avg Total Reward = 143.8\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "coach.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Running each training or inference iteration manually"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The graph manager (which was instantiated in the preset) can be accessed from the `CoachInterface` object. The graph manager simplifies the scheduling process by encapsulating the calls to each of the training phases. Sometimes, it can be beneficial to have a more fine grained control over the scheduling process. This can be easily done by calling the individual phase functions directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b]2;\u0007\n",
      "\u001b[30;46mCreating graph - name: BasicRLGraphManager\u001b[0m\n",
      "\u001b[30;46mCreating agent - name: agent\u001b[0m\n",
      "\u001b[30;46msimple_rl_graph: Starting heatup\u001b[0m\n",
      "\u001b[95mHeatup\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m1 \u001b[94mTotal reward: \u001b[0m19.0 \u001b[94mSteps: \u001b[0m19 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mHeatup\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m2 \u001b[94mTotal reward: \u001b[0m12.0 \u001b[94mSteps: \u001b[0m31 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mHeatup\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m3 \u001b[94mTotal reward: \u001b[0m15.0 \u001b[94mSteps: \u001b[0m46 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mHeatup\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m4 \u001b[94mTotal reward: \u001b[0m20.0 \u001b[94mSteps: \u001b[0m66 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mHeatup\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m5 \u001b[94mTotal reward: \u001b[0m21.0 \u001b[94mSteps: \u001b[0m87 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mHeatup\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m6 \u001b[94mTotal reward: \u001b[0m33.0 \u001b[94mSteps: \u001b[0m120 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m7 \u001b[94mTotal reward: \u001b[0m24.0 \u001b[94mSteps: \u001b[0m244 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m8 \u001b[94mTotal reward: \u001b[0m59.0 \u001b[94mSteps: \u001b[0m479 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m9 \u001b[94mTotal reward: \u001b[0m42.0 \u001b[94mSteps: \u001b[0m662 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m10 \u001b[94mTotal reward: \u001b[0m78.0 \u001b[94mSteps: \u001b[0m898 \u001b[94mTraining iteration: \u001b[0m0 \n"
     ]
    }
   ],
   "source": [
    "from rl_coach.environments.gym_environment import GymEnvironment, GymVectorEnvironment\n",
    "from rl_coach.base_parameters import VisualizationParameters\n",
    "from rl_coach.core_types import EnvironmentSteps\n",
    "\n",
    "tf.reset_default_graph()\n",
    "coach = CoachInterface(preset='CartPole_ClippedPPO')\n",
    "\n",
    "# registering an iteration signal before starting to run\n",
    "coach.graph_manager.log_signal('iteration', -1)\n",
    "\n",
    "coach.graph_manager.heatup(EnvironmentSteps(100))\n",
    "\n",
    "# training\n",
    "for it in range(10):\n",
    "    # logging the iteration signal during training\n",
    "    coach.graph_manager.log_signal('iteration', it)\n",
    "    # using the graph manager to train and act a given number of steps\n",
    "    coach.graph_manager.train_and_act(EnvironmentSteps(100))\n",
    "    # reading signals during training\n",
    "    training_reward = coach.graph_manager.get_signal_value('Training Reward')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes we may want to track the agent's decisions, log or maybe even modify them.\n",
    "We can access the agent itself through the `CoachInterface` as follows. \n",
    "\n",
    "Note that we also need an instance of the environment to do so. In this case we use instantiate a `GymEnvironment` object with the CartPole `GymVectorEnvironment`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State:{'observation': array([-0.03556981, -0.01759425, -0.00832918,  0.03255058])}, Action:1\n",
      "Reward:1.0\n",
      "State:{'observation': array([-0.0359217 ,  0.17764615, -0.00767817, -0.26274861])}, Action:0\n",
      "Reward:1.0\n",
      "State:{'observation': array([-0.03236878, -0.01736537, -0.01293314,  0.02750268])}, Action:1\n",
      "Reward:1.0\n",
      "State:{'observation': array([-0.03271608,  0.17793964, -0.01238309, -0.26923258])}, Action:0\n",
      "Reward:1.0\n",
      "State:{'observation': array([-0.02915729, -0.01700342, -0.01776774,  0.01951907])}, Action:0\n",
      "Reward:1.0\n",
      "State:{'observation': array([-0.02949736, -0.21186612, -0.01737736,  0.30654353])}, Action:1\n",
      "Reward:1.0\n",
      "State:{'observation': array([-0.03373468, -0.01650091, -0.01124649,  0.00843128])}, Action:0\n",
      "Reward:1.0\n",
      "State:{'observation': array([-0.0340647 , -0.21145978, -0.01107787,  0.29754469])}, Action:1\n",
      "Reward:1.0\n",
      "State:{'observation': array([-0.03829389, -0.01618168, -0.00512697,  0.00138869])}, Action:0\n",
      "Reward:1.0\n",
      "State:{'observation': array([-0.03861753, -0.21122973, -0.0050992 ,  0.29244959])}, Action:1\n",
      "Reward:1.0\n"
     ]
    }
   ],
   "source": [
    "# inference\n",
    "env_params = GymVectorEnvironment(level='CartPole-v0')\n",
    "env = GymEnvironment(**env_params.__dict__, visualization_parameters=VisualizationParameters())\n",
    "\n",
    "response = env.reset_internal_state()\n",
    "for _ in range(10):\n",
    "    action_info = coach.graph_manager.get_agent().choose_action(response.next_state)\n",
    "    print(\"State:{}, Action:{}\".format(response.next_state,action_info.action))\n",
    "    response = env.step(action_info.action)\n",
    "    print(\"Reward:{}\".format(response.reward))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-preset - using `GraphManager` directly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also possible to invoke coach directly in the python code without defining a preset (which is necessary for `CoachInterface`) by using the `GraphManager` object directly. Using Coach this way won't allow you access functionalities such as multi-threading, but it might be convenient if you don't want to define a preset file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training an agent with a custom Gym environment\n",
    "\n",
    "Here we show an example of how to use the `GraphManager` to train an agent on a custom Gym environment.\n",
    "\n",
    "We first construct a `GymEnvironmentParameters` object describing the environment parameters. For Gym environments with vector observations, we can use the more specific `GymVectorEnvironment` object. \n",
    "\n",
    "The path to the custom environment is defined in the `level` parameter and it can be the absolute path to its class (e.g. `'/home/user/my_environment_dir/my_environment_module.py:MyEnvironmentClass'`) or the relative path to the module as in this example. In any case, we can use the custom gym environment without registering it.\n",
    "\n",
    "Custom parameters for the environment's `__init__` function can be passed as `additional_simulator_parameters`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl_coach.agents.clipped_ppo_agent import ClippedPPOAgentParameters\n",
    "from rl_coach.environments.gym_environment import GymVectorEnvironment\n",
    "from rl_coach.graph_managers.basic_rl_graph_manager import BasicRLGraphManager\n",
    "from rl_coach.graph_managers.graph_manager import SimpleSchedule\n",
    "from rl_coach.architectures.embedder_parameters import InputEmbedderParameters\n",
    "\n",
    "#reseting tensorflow graph as we will use other network\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# define the environment parameters\n",
    "bit_length = 10\n",
    "env_params = GymVectorEnvironment(level='rl_coach.environments.toy_problems.bit_flip:BitFlip')\n",
    "env_params.additional_simulator_parameters = {'bit_length': bit_length, 'mean_zero': True}\n",
    "\n",
    "# Clipped PPO\n",
    "agent_params = ClippedPPOAgentParameters()\n",
    "agent_params.network_wrappers['main'].input_embedders_parameters = {\n",
    "    'state': InputEmbedderParameters(scheme=[]),\n",
    "    'desired_goal': InputEmbedderParameters(scheme=[])\n",
    "}\n",
    "\n",
    "graph_manager = BasicRLGraphManager(\n",
    "    agent_params=agent_params,\n",
    "    env_params=env_params,\n",
    "    schedule_params=SimpleSchedule()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[30;46mCreating graph - name: BasicRLGraphManager\u001b[0m\n",
      "\u001b[30;46mCreating agent - name: agent\u001b[0m\n",
      "\u001b[30;46mStarting to improve simple_rl_graph task index 0\u001b[0m\n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m1 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m10 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m2 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m20 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m3 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m30 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m4 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m40 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m5 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m50 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m6 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m60 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m7 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m70 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m8 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m80 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m9 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m90 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m10 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m100 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m11 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m110 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m12 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m120 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m13 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m130 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m14 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m140 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m15 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m150 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m16 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m160 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m17 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m170 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m18 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m180 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m19 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m190 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m20 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m200 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m21 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m210 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m22 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m220 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m23 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m230 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m24 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m240 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m25 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m250 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m26 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m260 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m27 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m270 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m28 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m280 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m29 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m290 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m30 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m300 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m31 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m310 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m32 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m320 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m33 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m330 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m34 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m340 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m35 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m350 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m36 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m360 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m37 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m370 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m38 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m380 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m39 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m390 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m40 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m400 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m41 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m410 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m42 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m420 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m43 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m430 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m44 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m440 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m45 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m450 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m46 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m460 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m47 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m470 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m48 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m480 \u001b[94mTraining iteration: \u001b[0m0 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m49 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m490 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m50 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m500 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[30;46magent: Starting evaluation phase\u001b[0m\n",
      "\u001b[95mTesting\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m50 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m500 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTesting\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m50 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m500 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTesting\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m50 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m500 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTesting\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m50 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m500 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTesting\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m50 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m500 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[30;46magent: Finished evaluation phase. Success rate = 0.0, Avg Total Reward = -10.0\u001b[0m\n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m51 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m510 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m52 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m520 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m53 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m530 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m54 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m540 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m55 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m550 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m56 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m560 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m57 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m570 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m58 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m580 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m59 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m590 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m60 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m600 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m61 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m610 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m62 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m620 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m63 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m630 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m64 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m640 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m65 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m650 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m66 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m660 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m67 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m670 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m68 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m680 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m69 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m690 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m70 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m700 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m71 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m710 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m72 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m720 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m73 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m730 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m74 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m740 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m75 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m750 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m76 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m760 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m77 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m770 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m78 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m780 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m79 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m790 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m80 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m800 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m81 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m810 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m82 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m820 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m83 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m830 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m84 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m840 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m85 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m850 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m86 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m860 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m87 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m870 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m88 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m880 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m89 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m890 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m90 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m900 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m91 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m910 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m92 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m920 \u001b[94mTraining iteration: \u001b[0m0 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m93 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m930 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m94 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m940 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m95 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m950 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m96 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m960 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m97 \u001b[94mTotal reward: \u001b[0m-6 \u001b[94mSteps: \u001b[0m967 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m98 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m977 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m99 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m987 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m100 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m997 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[30;46magent: Starting evaluation phase\u001b[0m\n",
      "\u001b[95mTesting\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m100 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m997 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTesting\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m100 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m997 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTesting\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m100 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m997 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTesting\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m100 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m997 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTesting\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m100 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m997 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[30;46magent: Finished evaluation phase. Success rate = 0.0, Avg Total Reward = -10.0\u001b[0m\n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m101 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m1007 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m102 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m1017 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m103 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m1027 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m104 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m1037 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m105 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m1047 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m106 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m1057 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m107 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m1067 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m108 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m1077 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m109 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m1087 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m110 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m1097 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m111 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m1107 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m112 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m1117 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m113 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m1127 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m114 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m1137 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m115 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m1147 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m116 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m1157 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m117 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m1167 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m118 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m1177 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m119 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m1187 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m120 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m1197 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m121 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m1207 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m122 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m1217 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m123 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m1227 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m124 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m1237 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m125 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m1247 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m126 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m1257 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m127 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m1267 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m128 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m1277 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m129 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m1287 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m130 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m1297 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m131 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m1307 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m132 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m1317 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m133 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m1327 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m134 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m1337 \u001b[94mTraining iteration: \u001b[0m0 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m135 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m1347 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m136 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m1357 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m137 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m1367 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m138 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m1377 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m139 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m1387 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m140 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m1397 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m141 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m1407 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m142 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m1417 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m143 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m1427 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m144 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m1437 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m145 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m1447 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m146 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m1457 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m147 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m1467 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m148 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m1477 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m149 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m1487 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m150 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m1497 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[30;46magent: Starting evaluation phase\u001b[0m\n",
      "\u001b[95mTesting\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m150 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m1497 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTesting\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m150 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m1497 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTesting\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m150 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m1497 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTesting\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m150 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m1497 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTesting\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m150 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m1497 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[30;46magent: Finished evaluation phase. Success rate = 0.0, Avg Total Reward = -10.0\u001b[0m\n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m151 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m1507 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m152 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m1517 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m153 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m1527 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m154 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m1537 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m155 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m1547 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m156 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m1557 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m157 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m1567 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m158 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m1577 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m159 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m1587 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m160 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m1597 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m161 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m1607 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m162 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m1617 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m163 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m1627 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m164 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m1637 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m165 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m1647 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m166 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m1657 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m167 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m1667 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m168 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m1677 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m169 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m1687 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m170 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m1697 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m171 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m1707 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m172 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m1717 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m173 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m1727 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m174 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m1737 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m175 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m1747 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m176 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m1757 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m177 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m1767 \u001b[94mTraining iteration: \u001b[0m0 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m178 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m1777 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m179 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m1787 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m180 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m1797 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m181 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m1807 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m182 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m1817 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m183 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m1827 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m184 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m1837 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m185 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m1847 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m186 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m1857 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m187 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m1867 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m188 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m1877 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m189 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m1887 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m190 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m1897 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m191 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m1907 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m192 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m1917 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m193 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m1927 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m194 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m1937 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m195 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m1947 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m196 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m1957 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m197 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m1967 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m198 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m1977 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m199 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m1987 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m200 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m1997 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[30;46magent: Starting evaluation phase\u001b[0m\n",
      "\u001b[95mTesting\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m200 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m1997 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTesting\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m200 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m1997 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTesting\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m200 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m1997 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTesting\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m200 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m1997 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTesting\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m200 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m1997 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[30;46magent: Finished evaluation phase. Success rate = 0.0, Avg Total Reward = -10.0\u001b[0m\n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m201 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m2007 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m202 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m2017 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m203 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m2027 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m204 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m2037 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m205 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m2047 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m206 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m2057 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mPolicy training\u001b[0m - \u001b[94mSurrogate loss: \u001b[0m0.0021487653721123934 \u001b[94mKL divergence: \u001b[0m0.0026376673486083746 \u001b[94mEntropy: \u001b[0m2.2463676929473877 \u001b[94mtraining epoch: \u001b[0m0 \u001b[94mlearning_rate: \u001b[0m0.00025 \n",
      "\u001b[95mPolicy training\u001b[0m - \u001b[94mSurrogate loss: \u001b[0m-0.021315602585673332 \u001b[94mKL divergence: \u001b[0m0.007733548991382122 \u001b[94mEntropy: \u001b[0m2.241731882095337 \u001b[94mtraining epoch: \u001b[0m1 \u001b[94mlearning_rate: \u001b[0m0.00025 \n",
      "\u001b[95mPolicy training\u001b[0m - \u001b[94mSurrogate loss: \u001b[0m-0.0276652742177248 \u001b[94mKL divergence: \u001b[0m0.01279161311686039 \u001b[94mEntropy: \u001b[0m2.236616611480713 \u001b[94mtraining epoch: \u001b[0m2 \u001b[94mlearning_rate: \u001b[0m0.00025 \n",
      "\u001b[95mPolicy training\u001b[0m - \u001b[94mSurrogate loss: \u001b[0m-0.029152197763323784 \u001b[94mKL divergence: \u001b[0m0.015343336388468742 \u001b[94mEntropy: \u001b[0m2.2353310585021973 \u001b[94mtraining epoch: \u001b[0m3 \u001b[94mlearning_rate: \u001b[0m0.00025 \n",
      "\u001b[95mPolicy training\u001b[0m - \u001b[94mSurrogate loss: \u001b[0m-0.030143793672323227 \u001b[94mKL divergence: \u001b[0m0.01678941212594509 \u001b[94mEntropy: \u001b[0m2.2330715656280518 \u001b[94mtraining epoch: \u001b[0m4 \u001b[94mlearning_rate: \u001b[0m0.00025 \n",
      "\u001b[95mPolicy training\u001b[0m - \u001b[94mSurrogate loss: \u001b[0m-0.031306106597185135 \u001b[94mKL divergence: \u001b[0m0.01814711093902588 \u001b[94mEntropy: \u001b[0m2.2306785583496094 \u001b[94mtraining epoch: \u001b[0m5 \u001b[94mlearning_rate: \u001b[0m0.00025 \n",
      "\u001b[95mPolicy training\u001b[0m - \u001b[94mSurrogate loss: \u001b[0m-0.03016328252851963 \u001b[94mKL divergence: \u001b[0m0.019376279786229134 \u001b[94mEntropy: \u001b[0m2.229426383972168 \u001b[94mtraining epoch: \u001b[0m6 \u001b[94mlearning_rate: \u001b[0m0.00025 \n",
      "\u001b[95mPolicy training\u001b[0m - \u001b[94mSurrogate loss: \u001b[0m-0.03084077313542366 \u001b[94mKL divergence: \u001b[0m0.020133696496486664 \u001b[94mEntropy: \u001b[0m2.229276180267334 \u001b[94mtraining epoch: \u001b[0m7 \u001b[94mlearning_rate: \u001b[0m0.00025 \n",
      "\u001b[95mPolicy training\u001b[0m - \u001b[94mSurrogate loss: \u001b[0m-0.031210508197546005 \u001b[94mKL divergence: \u001b[0m0.021088358014822006 \u001b[94mEntropy: \u001b[0m2.228335380554199 \u001b[94mtraining epoch: \u001b[0m8 \u001b[94mlearning_rate: \u001b[0m0.00025 \n",
      "\u001b[95mPolicy training\u001b[0m - \u001b[94mSurrogate loss: \u001b[0m-0.03182676434516907 \u001b[94mKL divergence: \u001b[0m0.021987974643707275 \u001b[94mEntropy: \u001b[0m2.2275402545928955 \u001b[94mtraining epoch: \u001b[0m9 \u001b[94mlearning_rate: \u001b[0m0.00025 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m207 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m2067 \u001b[94mTraining iteration: \u001b[0m1 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m208 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m2077 \u001b[94mTraining iteration: \u001b[0m1 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m209 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m2087 \u001b[94mTraining iteration: \u001b[0m1 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m210 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m2097 \u001b[94mTraining iteration: \u001b[0m1 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m211 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m2107 \u001b[94mTraining iteration: \u001b[0m1 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m212 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m2117 \u001b[94mTraining iteration: \u001b[0m1 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m213 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m2127 \u001b[94mTraining iteration: \u001b[0m1 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m214 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m2137 \u001b[94mTraining iteration: \u001b[0m1 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m215 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m2147 \u001b[94mTraining iteration: \u001b[0m1 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m216 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m2157 \u001b[94mTraining iteration: \u001b[0m1 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m217 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m2167 \u001b[94mTraining iteration: \u001b[0m1 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m218 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m2177 \u001b[94mTraining iteration: \u001b[0m1 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m219 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m2187 \u001b[94mTraining iteration: \u001b[0m1 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m220 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m2197 \u001b[94mTraining iteration: \u001b[0m1 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m221 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m2207 \u001b[94mTraining iteration: \u001b[0m1 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m222 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m2217 \u001b[94mTraining iteration: \u001b[0m1 \n",
      "\u001b[95mTraining\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m223 \u001b[94mTotal reward: \u001b[0m-10 \u001b[94mSteps: \u001b[0m2227 \u001b[94mTraining iteration: \u001b[0m1 \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-a2c628fc730f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#tf.reset_default_graph()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mgraph_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimprove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/rl_coach/graph_managers/graph_manager.py\u001b[0m in \u001b[0;36mimprove\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    547\u001b[0m         \u001b[0mcount_end\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal_steps_counters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mRunPhase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimprove_steps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal_steps_counters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mRunPhase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mcount_end\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 549\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_and_act\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps_between_evaluation_periods\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    550\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluation_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/rl_coach/graph_managers/graph_manager.py\u001b[0m in \u001b[0;36mtrain_and_act\u001b[0;34m(self, steps)\u001b[0m\n\u001b[1;32m    481\u001b[0m                     \u001b[0;31m# The agent might also decide to skip acting altogether.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    482\u001b[0m                     \u001b[0;31m# Depending on internal counters and parameters, it doesn't always train or save checkpoints.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 483\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEnvironmentSteps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    484\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    485\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moccasionally_save_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/rl_coach/graph_managers/graph_manager.py\u001b[0m in \u001b[0;36mact\u001b[0;34m(self, steps, wait_for_full_episodes)\u001b[0m\n\u001b[1;32m    447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m             \u001b[0msteps_begin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menvironments\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal_steps_counter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtop_level_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m             \u001b[0msteps_end\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menvironments\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal_steps_counter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/rl_coach/level_manager.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    264\u001b[0m             \u001b[0;31m# this is the agent's only opportunity to observe this transition - he will not get another one\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m             \u001b[0macting_agent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobserve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv_response\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# TODO: acting agent? maybe all of the agents in the layer?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_episode_ended\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    267\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_required\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/rl_coach/level_manager.py\u001b[0m in \u001b[0;36mhandle_episode_ended\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0;32mreturn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m         \"\"\"\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;34m[\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_episode_ended\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0magent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset_internal_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_environment_reset\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mEnvResponse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/rl_coach/level_manager.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0;32mreturn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m         \"\"\"\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;34m[\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_episode_ended\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0magent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset_internal_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_environment_reset\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mEnvResponse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/rl_coach/agents/agent.py\u001b[0m in \u001b[0;36mhandle_episode_ended\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    590\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0map\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvisualization\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump_csv\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparent_level_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparent_graph_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime_metric\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mTimeTypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEpisodeNumber\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 592\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_log\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    593\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    594\u001b[0m         \u001b[0;31m# TODO verbosity was mistakenly removed from task_parameters on release 0.11.0, need to bring it back\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/rl_coach/agents/agent.py\u001b[0m in \u001b[0;36mupdate_log\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magent_logger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_signal_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{}/Mean\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msignal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msignal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_mean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magent_logger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_signal_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{}/Stdev\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msignal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msignal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_stdev\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magent_logger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_signal_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{}/Max\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msignal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msignal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_max\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magent_logger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_signal_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{}/Min\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msignal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msignal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_min\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/rl_coach/logger.py\u001b[0m in \u001b[0;36mcreate_signal_value\u001b[0;34m(self, signal_name, value, overwrite, time)\u001b[0m\n\u001b[1;32m    196\u001b[0m         \u001b[0;31m# create only if it doesn't already exist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0moverwrite\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignal_value_exists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msignal_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 198\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msignal_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m    203\u001b[0m             \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_if_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_setitem_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setitem_with_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_validate_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_setitem_with_indexer\u001b[0;34m(self, indexer, value)\u001b[0m\n\u001b[1;32m    555\u001b[0m                 \u001b[0;31m# scalar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    556\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 557\u001b[0;31m                     \u001b[0msetter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    558\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36msetter\u001b[0;34m(item, v)\u001b[0m\n\u001b[1;32m    488\u001b[0m                     \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_consolidate_inplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m                     \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 490\u001b[0;31m                     \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    491\u001b[0m                     \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_update_cacher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclear\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36msetitem\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    558\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msetitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"setitem\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    561\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mputmask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, f, axes, filter, do_integrity_check, consolidate, **kwargs)\u001b[0m\n\u001b[1;32m    436\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb_items\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0malign_copy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 438\u001b[0;31m             \u001b[0mapplied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    439\u001b[0m             \u001b[0mresult_blocks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_extend_blocks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapplied\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult_blocks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pandas/core/internals/blocks.py\u001b[0m in \u001b[0;36msetitem\u001b[0;34m(self, indexer, value)\u001b[0m\n\u001b[1;32m    944\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtranspose\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    945\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 946\u001b[0;31m         \u001b[0mblock\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    947\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    948\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pandas/core/internals/blocks.py\u001b[0m in \u001b[0;36mmake_block\u001b[0;34m(self, values, placement)\u001b[0m\n\u001b[1;32m    263\u001b[0m             \u001b[0mplacement\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmgr_locs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmake_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplacement\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mplacement\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mndim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmake_block_same_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplacement\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mndim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pandas/core/internals/blocks.py\u001b[0m in \u001b[0;36mmake_block\u001b[0;34m(values, placement, klass, ndim, dtype, fastpath)\u001b[0m\n\u001b[1;32m   3265\u001b[0m         \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDatetimeArray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_simple_new\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3267\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mklass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mndim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplacement\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mplacement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3268\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3269\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pandas/core/internals/blocks.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, values, placement, ndim)\u001b[0m\n\u001b[1;32m   2769\u001b[0m     \u001b[0m_can_hold_na\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2770\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2771\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplacement\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mndim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2772\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0missubclass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2773\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "graph_manager.improve()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Advanced functionality - proprietary exploration policy, checkpoint evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agent modules, such as exploration policy, memory and neural network topology can be replaced with proprietary ones. In this example we'll show how to replace the default exploration policy of the DQN agent with a different one that is defined under the Resources folder. We'll also show how to change the default checkpoint save settings, and how to load a checkpoint for evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start with the standard definitions of a DQN agent solving the CartPole environment (taken from the Cartpole_DQN preset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl_coach.agents.dqn_agent import DQNAgentParameters\n",
    "from rl_coach.base_parameters import VisualizationParameters, TaskParameters\n",
    "from rl_coach.core_types import TrainingSteps, EnvironmentEpisodes, EnvironmentSteps\n",
    "from rl_coach.environments.gym_environment import GymVectorEnvironment\n",
    "from rl_coach.graph_managers.basic_rl_graph_manager import BasicRLGraphManager\n",
    "from rl_coach.graph_managers.graph_manager import ScheduleParameters\n",
    "from rl_coach.memories.memory import MemoryGranularity\n",
    "\n",
    "\n",
    "####################\n",
    "# Graph Scheduling #\n",
    "####################\n",
    "\n",
    "#reseting tensorflow graph as we will use other network\n",
    "tf.reset_default_graph()\n",
    "\n",
    "schedule_params = ScheduleParameters()\n",
    "schedule_params.improve_steps = TrainingSteps(4000)\n",
    "schedule_params.steps_between_evaluation_periods = EnvironmentEpisodes(10)\n",
    "schedule_params.evaluation_steps = EnvironmentEpisodes(1)\n",
    "schedule_params.heatup_steps = EnvironmentSteps(1000)\n",
    "\n",
    "#########\n",
    "# Agent #\n",
    "#########\n",
    "agent_params = DQNAgentParameters()\n",
    "\n",
    "# DQN params\n",
    "agent_params.algorithm.num_steps_between_copying_online_weights_to_target = EnvironmentSteps(100)\n",
    "agent_params.algorithm.discount = 0.99\n",
    "agent_params.algorithm.num_consecutive_playing_steps = EnvironmentSteps(1)\n",
    "\n",
    "# NN configuration\n",
    "agent_params.network_wrappers['main'].learning_rate = 0.00025\n",
    "agent_params.network_wrappers['main'].replace_mse_with_huber_loss = False\n",
    "\n",
    "# ER size\n",
    "agent_params.memory.max_size = (MemoryGranularity.Transitions, 40000)\n",
    "\n",
    "################\n",
    "#  Environment #\n",
    "################\n",
    "env_params = GymVectorEnvironment(level='CartPole-v0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll override the exploration policy with our own policy defined in `Resources/exploration.py`.\n",
    "We'll also define the checkpoint save directory and interval in seconds.\n",
    "\n",
    "Make sure the first cell at the top of this notebook is run before the following one, such that module_path and resources_path are adding to sys path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[30;46mCreating graph - name: BasicRLGraphManager\u001b[0m\n",
      "\u001b[30;46mCreating agent - name: agent\u001b[0m\n",
      "\u001b[30;46msimple_rl_graph: Starting heatup\u001b[0m\n",
      "\u001b[95mHeatup\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m1 \u001b[94mTotal reward: \u001b[0m24.0 \u001b[94mExploration: \u001b[0m0 \u001b[94mSteps: \u001b[0m24 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mHeatup\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m2 \u001b[94mTotal reward: \u001b[0m12.0 \u001b[94mExploration: \u001b[0m0 \u001b[94mSteps: \u001b[0m36 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mHeatup\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m3 \u001b[94mTotal reward: \u001b[0m15.0 \u001b[94mExploration: \u001b[0m0 \u001b[94mSteps: \u001b[0m51 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mHeatup\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m4 \u001b[94mTotal reward: \u001b[0m53.0 \u001b[94mExploration: \u001b[0m0 \u001b[94mSteps: \u001b[0m104 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mHeatup\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m5 \u001b[94mTotal reward: \u001b[0m17.0 \u001b[94mExploration: \u001b[0m0 \u001b[94mSteps: \u001b[0m121 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mHeatup\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m6 \u001b[94mTotal reward: \u001b[0m17.0 \u001b[94mExploration: \u001b[0m0 \u001b[94mSteps: \u001b[0m138 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mHeatup\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m7 \u001b[94mTotal reward: \u001b[0m16.0 \u001b[94mExploration: \u001b[0m0 \u001b[94mSteps: \u001b[0m154 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mHeatup\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m8 \u001b[94mTotal reward: \u001b[0m23.0 \u001b[94mExploration: \u001b[0m0 \u001b[94mSteps: \u001b[0m177 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mHeatup\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m9 \u001b[94mTotal reward: \u001b[0m13.0 \u001b[94mExploration: \u001b[0m0 \u001b[94mSteps: \u001b[0m190 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mHeatup\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m10 \u001b[94mTotal reward: \u001b[0m19.0 \u001b[94mExploration: \u001b[0m0 \u001b[94mSteps: \u001b[0m209 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mHeatup\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m11 \u001b[94mTotal reward: \u001b[0m41.0 \u001b[94mExploration: \u001b[0m0 \u001b[94mSteps: \u001b[0m250 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mHeatup\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m12 \u001b[94mTotal reward: \u001b[0m19.0 \u001b[94mExploration: \u001b[0m0 \u001b[94mSteps: \u001b[0m269 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mHeatup\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m13 \u001b[94mTotal reward: \u001b[0m11.0 \u001b[94mExploration: \u001b[0m0 \u001b[94mSteps: \u001b[0m280 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mHeatup\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m14 \u001b[94mTotal reward: \u001b[0m11.0 \u001b[94mExploration: \u001b[0m0 \u001b[94mSteps: \u001b[0m291 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mHeatup\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m15 \u001b[94mTotal reward: \u001b[0m13.0 \u001b[94mExploration: \u001b[0m0 \u001b[94mSteps: \u001b[0m304 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mHeatup\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m16 \u001b[94mTotal reward: \u001b[0m19.0 \u001b[94mExploration: \u001b[0m0 \u001b[94mSteps: \u001b[0m323 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mHeatup\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m17 \u001b[94mTotal reward: \u001b[0m20.0 \u001b[94mExploration: \u001b[0m0 \u001b[94mSteps: \u001b[0m343 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mHeatup\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m18 \u001b[94mTotal reward: \u001b[0m10.0 \u001b[94mExploration: \u001b[0m0 \u001b[94mSteps: \u001b[0m353 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mHeatup\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m19 \u001b[94mTotal reward: \u001b[0m10.0 \u001b[94mExploration: \u001b[0m0 \u001b[94mSteps: \u001b[0m363 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mHeatup\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m20 \u001b[94mTotal reward: \u001b[0m22.0 \u001b[94mExploration: \u001b[0m0 \u001b[94mSteps: \u001b[0m385 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mHeatup\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m21 \u001b[94mTotal reward: \u001b[0m20.0 \u001b[94mExploration: \u001b[0m0 \u001b[94mSteps: \u001b[0m405 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mHeatup\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m22 \u001b[94mTotal reward: \u001b[0m20.0 \u001b[94mExploration: \u001b[0m0 \u001b[94mSteps: \u001b[0m425 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mHeatup\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m23 \u001b[94mTotal reward: \u001b[0m11.0 \u001b[94mExploration: \u001b[0m0 \u001b[94mSteps: \u001b[0m436 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mHeatup\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m24 \u001b[94mTotal reward: \u001b[0m13.0 \u001b[94mExploration: \u001b[0m0 \u001b[94mSteps: \u001b[0m449 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mHeatup\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m25 \u001b[94mTotal reward: \u001b[0m21.0 \u001b[94mExploration: \u001b[0m0 \u001b[94mSteps: \u001b[0m470 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mHeatup\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m26 \u001b[94mTotal reward: \u001b[0m16.0 \u001b[94mExploration: \u001b[0m0 \u001b[94mSteps: \u001b[0m486 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mHeatup\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m27 \u001b[94mTotal reward: \u001b[0m12.0 \u001b[94mExploration: \u001b[0m0 \u001b[94mSteps: \u001b[0m498 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mHeatup\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m28 \u001b[94mTotal reward: \u001b[0m32.0 \u001b[94mExploration: \u001b[0m0 \u001b[94mSteps: \u001b[0m530 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mHeatup\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m29 \u001b[94mTotal reward: \u001b[0m25.0 \u001b[94mExploration: \u001b[0m0 \u001b[94mSteps: \u001b[0m555 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mHeatup\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m30 \u001b[94mTotal reward: \u001b[0m22.0 \u001b[94mExploration: \u001b[0m0 \u001b[94mSteps: \u001b[0m577 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mHeatup\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m31 \u001b[94mTotal reward: \u001b[0m17.0 \u001b[94mExploration: \u001b[0m0 \u001b[94mSteps: \u001b[0m594 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mHeatup\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m32 \u001b[94mTotal reward: \u001b[0m11.0 \u001b[94mExploration: \u001b[0m0 \u001b[94mSteps: \u001b[0m605 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mHeatup\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m33 \u001b[94mTotal reward: \u001b[0m24.0 \u001b[94mExploration: \u001b[0m0 \u001b[94mSteps: \u001b[0m629 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mHeatup\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m34 \u001b[94mTotal reward: \u001b[0m21.0 \u001b[94mExploration: \u001b[0m0 \u001b[94mSteps: \u001b[0m650 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mHeatup\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m35 \u001b[94mTotal reward: \u001b[0m22.0 \u001b[94mExploration: \u001b[0m0 \u001b[94mSteps: \u001b[0m672 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mHeatup\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m36 \u001b[94mTotal reward: \u001b[0m16.0 \u001b[94mExploration: \u001b[0m0 \u001b[94mSteps: \u001b[0m688 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mHeatup\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m37 \u001b[94mTotal reward: \u001b[0m11.0 \u001b[94mExploration: \u001b[0m0 \u001b[94mSteps: \u001b[0m699 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mHeatup\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m38 \u001b[94mTotal reward: \u001b[0m13.0 \u001b[94mExploration: \u001b[0m0 \u001b[94mSteps: \u001b[0m712 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mHeatup\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m39 \u001b[94mTotal reward: \u001b[0m35.0 \u001b[94mExploration: \u001b[0m0 \u001b[94mSteps: \u001b[0m747 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mHeatup\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m40 \u001b[94mTotal reward: \u001b[0m24.0 \u001b[94mExploration: \u001b[0m0 \u001b[94mSteps: \u001b[0m771 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[95mHeatup\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m41 \u001b[94mTotal reward: \u001b[0m17.0 \u001b[94mExploration: \u001b[0m0 \u001b[94mSteps: \u001b[0m788 \u001b[94mTraining iteration: \u001b[0m0 \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-502d000d9231>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mgraph_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask_parameters1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mgraph_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimprove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/rl_coach/graph_managers/graph_manager.py\u001b[0m in \u001b[0;36mimprove\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    537\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    538\u001b[0m         \u001b[0;31m# heatup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 539\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheatup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheatup_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    540\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    541\u001b[0m         \u001b[0;31m# improve\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/rl_coach/graph_managers/graph_manager.py\u001b[0m in \u001b[0;36mheatup\u001b[0;34m(self, steps)\u001b[0m\n\u001b[1;32m    387\u001b[0m                 \u001b[0mcount_end\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_step_counter\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    388\u001b[0m                 \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_step_counter\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mcount_end\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 389\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEnvironmentEpisodes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mhandle_episode_ended\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/rl_coach/graph_managers/graph_manager.py\u001b[0m in \u001b[0;36mact\u001b[0;34m(self, steps, wait_for_full_episodes)\u001b[0m\n\u001b[1;32m    447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m             \u001b[0msteps_begin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menvironments\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal_steps_counter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtop_level_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m             \u001b[0msteps_end\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menvironments\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal_steps_counter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/rl_coach/level_manager.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    264\u001b[0m             \u001b[0;31m# this is the agent's only opportunity to observe this transition - he will not get another one\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m             \u001b[0macting_agent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobserve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv_response\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# TODO: acting agent? maybe all of the agents in the layer?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_episode_ended\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    267\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_required\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/rl_coach/level_manager.py\u001b[0m in \u001b[0;36mhandle_episode_ended\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0;32mreturn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m         \"\"\"\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;34m[\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_episode_ended\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0magent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset_internal_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_environment_reset\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mEnvResponse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/rl_coach/level_manager.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0;32mreturn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m         \"\"\"\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;34m[\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_episode_ended\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0magent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset_internal_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_environment_reset\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mEnvResponse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/rl_coach/agents/agent.py\u001b[0m in \u001b[0;36mhandle_episode_ended\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    590\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0map\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvisualization\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump_csv\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparent_level_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparent_graph_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime_metric\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mTimeTypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEpisodeNumber\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 592\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_log\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    593\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    594\u001b[0m         \u001b[0;31m# TODO verbosity was mistakenly removed from task_parameters on release 0.11.0, need to bring it back\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/rl_coach/agents/agent.py\u001b[0m in \u001b[0;36mupdate_log\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    546\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0msignal\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepisode_signals\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 548\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magent_logger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_signal_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{}/Mean\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msignal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msignal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_mean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    549\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magent_logger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_signal_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{}/Stdev\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msignal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msignal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_stdev\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magent_logger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_signal_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{}/Max\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msignal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msignal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_max\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from exploration import MyExplorationParameters\n",
    "\n",
    "# Overriding the default DQN Agent exploration policy with my exploration policy\n",
    "agent_params.exploration = MyExplorationParameters()\n",
    "\n",
    "# Creating a graph manager to train a DQN agent to solve CartPole\n",
    "graph_manager = BasicRLGraphManager(agent_params=agent_params, env_params=env_params,\n",
    "                                    schedule_params=schedule_params, vis_params=VisualizationParameters())\n",
    "\n",
    "# Resources path was defined at the top of this notebook\n",
    "my_checkpoint_dir = resources_path + '/checkpoints'\n",
    "\n",
    "# Checkpoints will be stored every 5 seconds to the given directory\n",
    "task_parameters1 = TaskParameters()\n",
    "task_parameters1.checkpoint_save_dir = my_checkpoint_dir\n",
    "task_parameters1.checkpoint_save_secs = 5\n",
    "\n",
    "graph_manager.create_graph(task_parameters1)\n",
    "graph_manager.improve()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last, we'll load the latest checkpoint from the checkpoint directory, and evaluate it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[30;46magent: Starting evaluation phase\u001b[0m\n",
      "\u001b[95mTesting\u001b[0m - \u001b[94mName: \u001b[0mmain_level/agent \u001b[94mWorker: \u001b[0m0 \u001b[94mEpisode: \u001b[0m42 \u001b[94mTotal reward: \u001b[0m10.0 \u001b[94mExploration: \u001b[0m0 \u001b[94mSteps: \u001b[0m804 \u001b[94mTraining iteration: \u001b[0m0 \n",
      "\u001b[30;46magent: Finished evaluation phase. Success rate = 0.0, Avg Total Reward = 10.0\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import shutil\n",
    "\n",
    "# Clearing the previous graph before creating the new one to avoid name conflicts\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Updating the graph manager's task parameters to restore the latest stored checkpoint from the checkpoints directory\n",
    "task_parameters2 = TaskParameters()\n",
    "task_parameters2.checkpoint_restore_path = my_checkpoint_dir\n",
    "\n",
    "graph_manager.create_graph(task_parameters2)\n",
    "graph_manager.evaluate(EnvironmentSteps(5))\n",
    "\n",
    "# Clearning up\n",
    "shutil.rmtree(my_checkpoint_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
